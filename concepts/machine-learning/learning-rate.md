# Learning Rate

[[deep-learning]] [[artificial-neural-networks]] are trained using the stochastic [[gradient-descent]] optimization algorithm.

The learning rate is a hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated. Choosing the learning rate is challenging as a value too small may result in a long training process that could get stuck, whereas a value too large may result in learning a sub-optimal set of weights too fast or an unstable training process.

The learning rate may be the most important hyperparameter when configuring your neural network. Therefore it is vital to know how to investigate the effects of the learning rate on model performance and to build an intuition about the dynamics of the learning rate on model behavior.

## Useful Links

- [Understand the Impact of Learning Rate on Neural Network Performance](https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks)

[//begin]: # "Autogenerated link references for markdown compatibility"
[deep-learning]: deep-learning "Deep Learning"
[artificial-neural-networks]: artificial-neural-networks "Artificial Neural Networks"
[gradient-descent]: gradient-descent "Gradient Descent"
[//end]: # "Autogenerated link references"
